{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\End_To_End_project\\\\Ham_Spam_Classifier(ETE)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"artifact\\01_25_2024_15_11_32\\data_validation\\invalid\\invalidated_data.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *\n",
    "from src.models.label_encoding import LabelConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text_Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                                       Text_Message\n",
       "0       0  Go until jurong point, crazy.. Available only ...\n",
       "1       0                      Ok lar... Joking wif u oni...\n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       0  U dun say so early hor... U c already then say...\n",
       "4       0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_converter = LabelConverter()\n",
    "df[\"Target\"] = df[\"Target\"].apply(lambda x: label_converter.ham if x == 'ham' else label_converter.spam)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = df[df.Target ==1]\n",
    "ham = df[df.Target ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text_Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>1</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>1</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>1</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>747 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target                                       Text_Message\n",
       "2          1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "5          1  FreeMsg Hey there darling it's been 3 week's n...\n",
       "8          1  WINNER!! As a valued network customer you have...\n",
       "9          1  Had your mobile 11 months or more? U R entitle...\n",
       "11         1  SIX chances to win CASH! From 100 to 20,000 po...\n",
       "...      ...                                                ...\n",
       "5537       1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "5540       1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "5547       1  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5566       1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567       1  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[747 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text_Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>0</td>\n",
       "      <td>Huh y lei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4825 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target                                       Text_Message\n",
       "0          0  Go until jurong point, crazy.. Available only ...\n",
       "1          0                      Ok lar... Joking wif u oni...\n",
       "3          0  U dun say so early hor... U c already then say...\n",
       "4          0  Nah I don't think he goes to usf, he lives aro...\n",
       "6          0  Even my brother is not like to speak with me. ...\n",
       "...      ...                                                ...\n",
       "5565       0                                       Huh y lei...\n",
       "5568       0              Will Ì_ b going to esplanade fr home?\n",
       "5569       0  Pity, * was in mood for that. So...any other s...\n",
       "5570       0  The guy did some bitching but I acted like i'd...\n",
       "5571       0                         Rofl. Its true to its name\n",
       "\n",
       "[4825 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.Text_Message.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"free entri 2 week comp win fa cup final tkts 21st may 2005. text fa 87121 receiv entri question ( std txt rate ) & c 's appli 08452810075over18 's\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning(spam.Text_Message.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_data\"] = [data_cleaning(i) for i in df.Text_Message.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text_Message</th>\n",
       "      <th>clean_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point , crazi .. avail bugi n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar ... joke oni ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri 2 week comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>dun say earli hor ... c alreadi say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah n't think goe usf , live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>2nd time tri 2 contact . å£750 pound prize . 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>ì_ b go esplanad fr home ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>piti , * mood . ... suggest ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>guy bitch act like 'd interest buy someth els ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>roll floor laugh . true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target                                       Text_Message  \\\n",
       "0          0  Go until jurong point, crazy.. Available only ...   \n",
       "1          0                      Ok lar... Joking wif u oni...   \n",
       "2          1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3          0  U dun say so early hor... U c already then say...   \n",
       "4          0  Nah I don't think he goes to usf, he lives aro...   \n",
       "...      ...                                                ...   \n",
       "5567       1  This is the 2nd time we have tried 2 contact u...   \n",
       "5568       0              Will Ì_ b going to esplanade fr home?   \n",
       "5569       0  Pity, * was in mood for that. So...any other s...   \n",
       "5570       0  The guy did some bitching but I acted like i'd...   \n",
       "5571       0                         Rofl. Its true to its name   \n",
       "\n",
       "                                             clean_data  \n",
       "0     go jurong point , crazi .. avail bugi n great ...  \n",
       "1                               ok lar ... joke oni ...  \n",
       "2     free entri 2 week comp win fa cup final tkts 2...  \n",
       "3               dun say earli hor ... c alreadi say ...  \n",
       "4            nah n't think goe usf , live around though  \n",
       "...                                                 ...  \n",
       "5567  2nd time tri 2 contact . å£750 pound prize . 2...  \n",
       "5568                         ì_ b go esplanad fr home ?  \n",
       "5569                      piti , * mood . ... suggest ?  \n",
       "5570  guy bitch act like 'd interest buy someth els ...  \n",
       "5571                       roll floor laugh . true name  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['tokenized_data'] = df['clean_data'].apply(word_tokenize)\n",
    "df['word_length'] = [len(i) for i in df['tokenized_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text_Message</th>\n",
       "      <th>clean_data</th>\n",
       "      <th>tokenized_data</th>\n",
       "      <th>word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point , crazi .. avail bugi n great ...</td>\n",
       "      <td>[go, jurong, point, ,, crazi, .., avail, bugi,...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar ... joke oni ...</td>\n",
       "      <td>[ok, lar, ..., joke, oni, ...]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri 2 week comp win fa cup final tkts 2...</td>\n",
       "      <td>[free, entri, 2, week, comp, win, fa, cup, fin...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>dun say earli hor ... c alreadi say ...</td>\n",
       "      <td>[dun, say, earli, hor, ..., c, alreadi, say, ...]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah n't think goe usf , live around though</td>\n",
       "      <td>[nah, n't, think, goe, usf, ,, live, around, t...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>2nd time tri 2 contact . å£750 pound prize . 2...</td>\n",
       "      <td>[2nd, time, tri, 2, contact, ., å£750, pound, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>ì_ b go esplanad fr home ?</td>\n",
       "      <td>[ì_, b, go, esplanad, fr, home, ?]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>piti , * mood . ... suggest ?</td>\n",
       "      <td>[piti, ,, *, mood, ., ..., suggest, ?]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>guy bitch act like 'd interest buy someth els ...</td>\n",
       "      <td>[guy, bitch, act, like, 'd, interest, buy, som...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>roll floor laugh . true name</td>\n",
       "      <td>[roll, floor, laugh, ., true, name]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target                                       Text_Message  \\\n",
       "0          0  Go until jurong point, crazy.. Available only ...   \n",
       "1          0                      Ok lar... Joking wif u oni...   \n",
       "2          1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3          0  U dun say so early hor... U c already then say...   \n",
       "4          0  Nah I don't think he goes to usf, he lives aro...   \n",
       "...      ...                                                ...   \n",
       "5567       1  This is the 2nd time we have tried 2 contact u...   \n",
       "5568       0              Will Ì_ b going to esplanade fr home?   \n",
       "5569       0  Pity, * was in mood for that. So...any other s...   \n",
       "5570       0  The guy did some bitching but I acted like i'd...   \n",
       "5571       0                         Rofl. Its true to its name   \n",
       "\n",
       "                                             clean_data  \\\n",
       "0     go jurong point , crazi .. avail bugi n great ...   \n",
       "1                               ok lar ... joke oni ...   \n",
       "2     free entri 2 week comp win fa cup final tkts 2...   \n",
       "3               dun say earli hor ... c alreadi say ...   \n",
       "4            nah n't think goe usf , live around though   \n",
       "...                                                 ...   \n",
       "5567  2nd time tri 2 contact . å£750 pound prize . 2...   \n",
       "5568                         ì_ b go esplanad fr home ?   \n",
       "5569                      piti , * mood . ... suggest ?   \n",
       "5570  guy bitch act like 'd interest buy someth els ...   \n",
       "5571                       roll floor laugh . true name   \n",
       "\n",
       "                                         tokenized_data  word_length  \n",
       "0     [go, jurong, point, ,, crazi, .., avail, bugi,...           20  \n",
       "1                        [ok, lar, ..., joke, oni, ...]            6  \n",
       "2     [free, entri, 2, week, comp, win, fa, cup, fin...           30  \n",
       "3     [dun, say, earli, hor, ..., c, alreadi, say, ...]            9  \n",
       "4     [nah, n't, think, goe, usf, ,, live, around, t...            9  \n",
       "...                                                 ...          ...  \n",
       "5567  [2nd, time, tri, 2, contact, ., å£750, pound, ...           24  \n",
       "5568                 [ì_, b, go, esplanad, fr, home, ?]            7  \n",
       "5569             [piti, ,, *, mood, ., ..., suggest, ?]            8  \n",
       "5570  [guy, bitch, act, like, 'd, interest, buy, som...           14  \n",
       "5571                [roll, floor, laugh, ., true, name]            6  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by='word_length', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5268                                                [\\er]\n",
       "5188                                                [oki]\n",
       "3049                                                 [ok]\n",
       "1924                                                 [ok]\n",
       "3092    [staff.science.nus.edu.sg/~phyhcmk/teaching/pc...\n",
       "                              ...                        \n",
       "1862    [last, thing, ever, want, hurt, ., n't, think,...\n",
       "2157    [sad, stori, man, -, last, week, b'day, ., wif...\n",
       "2847    [sad, stori, man, -, last, week, b'day, ., wif...\n",
       "2433    [indian, poor, india, poor, countri, ., say, o...\n",
       "1578    [make, girl, happi, ?, 's, difficult, make, gi...\n",
       "Name: tokenized_data, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df.tokenized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 43, 44, 44, 44, 45, 45, 46, 46, 47, 47, 47, 47, 48, 48, 48, 49, 50, 50, 51, 51, 51, 51, 51, 52, 52, 53, 53, 56, 56, 56, 56, 58, 59, 60, 60, 61, 64, 82, 85, 86, 86, 87, 87, 87, 96, 99, 99, 124, 202'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(map(str, sorted_df['word_length'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5268                                                [\\er]\n",
       "5188                                                [oki]\n",
       "3049                                                 [ok]\n",
       "1924                                                 [ok]\n",
       "3092    [staff.science.nus.edu.sg/~phyhcmk/teaching/pc...\n",
       "                              ...                        \n",
       "1862    [last, thing, ever, want, hurt, ., n't, think,...\n",
       "2157    [sad, stori, man, -, last, week, b'day, ., wif...\n",
       "2847    [sad, stori, man, -, last, week, b'day, ., wif...\n",
       "2433    [indian, poor, india, poor, countri, ., say, o...\n",
       "1578    [make, girl, happi, ?, 's, difficult, make, gi...\n",
       "Name: tokenized_data, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df.tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\er',\n",
       " 'oki',\n",
       " 'ok',\n",
       " 'ok',\n",
       " 'staff.science.nus.edu.sg/~phyhcmk/teaching/pc1323',\n",
       " 'yup',\n",
       " 'oki',\n",
       " '@',\n",
       " 'ok',\n",
       " '...',\n",
       " 'phone',\n",
       " '6time',\n",
       " 'hungri',\n",
       " 'yup',\n",
       " 'food',\n",
       " 'respond',\n",
       " 'alrit',\n",
       " '...',\n",
       " 'idea',\n",
       " 'g.w.are',\n",
       " 'ok',\n",
       " '645',\n",
       " 'oki',\n",
       " 'oki',\n",
       " 'pock',\n",
       " 'like',\n",
       " '?',\n",
       " 'number',\n",
       " 'sir',\n",
       " '.',\n",
       " 'happen',\n",
       " 'ok',\n",
       " '...',\n",
       " 'buy',\n",
       " '.',\n",
       " 'brother',\n",
       " 'genius',\n",
       " 'talk',\n",
       " '..',\n",
       " 'plan',\n",
       " '?',\n",
       " 'late',\n",
       " '.',\n",
       " '\\\\hey',\n",
       " 'kate',\n",
       " 'movi',\n",
       " 'laptop',\n",
       " 'look',\n",
       " '?',\n",
       " 'ok',\n",
       " '.',\n",
       " 'come',\n",
       " 'home',\n",
       " 'ok',\n",
       " '...',\n",
       " 'today',\n",
       " '?',\n",
       " 'nite',\n",
       " '...',\n",
       " 'know',\n",
       " 'call',\n",
       " 'good',\n",
       " 'hear',\n",
       " ':',\n",
       " ')',\n",
       " 'wana',\n",
       " 'see',\n",
       " 'ever',\n",
       " 'easier',\n",
       " 'happi',\n",
       " 'say',\n",
       " 'see',\n",
       " '!',\n",
       " 'meet',\n",
       " '?',\n",
       " 'sis',\n",
       " '?',\n",
       " 'never',\n",
       " 'noth',\n",
       " 'convey',\n",
       " 'regard',\n",
       " 'anyth',\n",
       " '...',\n",
       " 'way',\n",
       " 'home',\n",
       " 'b4',\n",
       " 'thursday',\n",
       " 'unbeliev',\n",
       " 'faglord',\n",
       " '2',\n",
       " '.',\n",
       " 'drop',\n",
       " 'tank',\n",
       " 'eye',\n",
       " 'philosophi',\n",
       " 'wat',\n",
       " '?',\n",
       " 'lectur',\n",
       " '?',\n",
       " 'yeah',\n",
       " 'wors',\n",
       " 'much',\n",
       " 'buzi',\n",
       " 'ok',\n",
       " '...',\n",
       " 'much',\n",
       " 'buzi',\n",
       " 'k.',\n",
       " 'sent',\n",
       " 'bluray',\n",
       " 'player',\n",
       " 'way',\n",
       " '.',\n",
       " 'cough',\n",
       " 'noth',\n",
       " 'beerag',\n",
       " '?',\n",
       " 'contin',\n",
       " '?',\n",
       " 'ok',\n",
       " '.',\n",
       " 'hold',\n",
       " '?',\n",
       " 'mani',\n",
       " 'depend',\n",
       " 'pizza',\n",
       " 'want',\n",
       " '?',\n",
       " '?',\n",
       " 'anytim',\n",
       " '...',\n",
       " 'yes',\n",
       " 'fine',\n",
       " 'special',\n",
       " '?',\n",
       " 'ok',\n",
       " 'prob',\n",
       " 'start',\n",
       " 'skye',\n",
       " 'oki',\n",
       " '...',\n",
       " 'late',\n",
       " '.',\n",
       " 'get',\n",
       " '?',\n",
       " 'call',\n",
       " '...',\n",
       " 'number',\n",
       " 'sir',\n",
       " '.',\n",
       " 'rememb',\n",
       " '.',\n",
       " 'check',\n",
       " 'erutupalam',\n",
       " 'thandiyachu',\n",
       " 'stand',\n",
       " '.',\n",
       " 'havent',\n",
       " '.',\n",
       " 'stupid.it',\n",
       " 'possibl',\n",
       " 'tell',\n",
       " 'address',\n",
       " 'togeth',\n",
       " 'thinkin',\n",
       " 'say',\n",
       " 'happen',\n",
       " 'way',\n",
       " 'home',\n",
       " '?',\n",
       " '?',\n",
       " 'thank',\n",
       " '!',\n",
       " 'womdarful',\n",
       " 'actor',\n",
       " 'head',\n",
       " 'straight',\n",
       " 'come',\n",
       " 'peopl',\n",
       " 'stupid.it',\n",
       " 'possibl',\n",
       " 'want',\n",
       " 'talk',\n",
       " 'made',\n",
       " 'appoint',\n",
       " 'when/wher',\n",
       " 'pick',\n",
       " 'get',\n",
       " 'librari',\n",
       " 'movi',\n",
       " 'laptop',\n",
       " 'wat',\n",
       " '?',\n",
       " 'wish',\n",
       " '!',\n",
       " 'one',\n",
       " 'talk',\n",
       " 'ok',\n",
       " '...',\n",
       " 'stitch',\n",
       " 'trouser',\n",
       " 'thanx',\n",
       " '...',\n",
       " '2/2',\n",
       " '146tf150p',\n",
       " 'earli',\n",
       " '?',\n",
       " 'call',\n",
       " '.',\n",
       " 'wat',\n",
       " '?',\n",
       " 'oki',\n",
       " '...',\n",
       " 'ok',\n",
       " '..',\n",
       " 'ok',\n",
       " '.',\n",
       " 'get',\n",
       " 'messag',\n",
       " 'much',\n",
       " 'buzi',\n",
       " 'plan',\n",
       " 'manag',\n",
       " 'place',\n",
       " 'man',\n",
       " 'ringtonek',\n",
       " '84484',\n",
       " 'convey',\n",
       " 'regard',\n",
       " '26th',\n",
       " 'juli',\n",
       " 'get',\n",
       " 'home',\n",
       " 'ok',\n",
       " '...',\n",
       " 'you_know',\n",
       " '...',\n",
       " 'exact',\n",
       " 'intent',\n",
       " 'k.',\n",
       " 'sent',\n",
       " 'donno',\n",
       " 'scorabl',\n",
       " 'happen',\n",
       " 'adventur',\n",
       " 'txting',\n",
       " 'drive',\n",
       " 'late',\n",
       " '.',\n",
       " 'one',\n",
       " '.',\n",
       " 'convey',\n",
       " 'regard',\n",
       " 'wat',\n",
       " '?',\n",
       " 'haha',\n",
       " 'think',\n",
       " 'somewher',\n",
       " 'fredericksburg',\n",
       " 'wait',\n",
       " 'yesterday',\n",
       " 'leav',\n",
       " '.',\n",
       " 'fps',\n",
       " '.',\n",
       " 'dear',\n",
       " 'reach',\n",
       " 'midnight',\n",
       " 'earliest',\n",
       " 'place',\n",
       " 'man',\n",
       " 'guy',\n",
       " '.',\n",
       " 'ok',\n",
       " '...',\n",
       " 'place',\n",
       " 'man',\n",
       " 'ok',\n",
       " '...',\n",
       " '\\\\alright',\n",
       " 'babe',\n",
       " 'oki',\n",
       " '...',\n",
       " 'stalk',\n",
       " '?',\n",
       " 'ok',\n",
       " '...',\n",
       " 'scratch',\n",
       " '?',\n",
       " 'home',\n",
       " 'way',\n",
       " 'carri',\n",
       " 'disturb',\n",
       " 'come',\n",
       " '?',\n",
       " '*',\n",
       " 'way',\n",
       " 'call',\n",
       " '...',\n",
       " 'tire',\n",
       " 'special',\n",
       " 'tell',\n",
       " 'reach',\n",
       " 'ok',\n",
       " '...',\n",
       " 'come',\n",
       " '?',\n",
       " 'wondarful',\n",
       " 'song',\n",
       " 'got',\n",
       " 'ta',\n",
       " 'east',\n",
       " 'coast',\n",
       " 'mayb',\n",
       " 'pressi',\n",
       " 'duck',\n",
       " 'chinchilla',\n",
       " 'marriag',\n",
       " 'function',\n",
       " 'lei',\n",
       " '?',\n",
       " 'ok',\n",
       " '...',\n",
       " 'ok',\n",
       " '...',\n",
       " 'room',\n",
       " '.',\n",
       " 'k.',\n",
       " 'sent',\n",
       " 'ok',\n",
       " '.',\n",
       " 'talk',\n",
       " '?',\n",
       " 'bring',\n",
       " 'got',\n",
       " 'see',\n",
       " '?',\n",
       " 'stupid.it',\n",
       " 'possibl',\n",
       " 'yup',\n",
       " '...',\n",
       " 'avatar',\n",
       " 'suppos',\n",
       " 'subtoitl',\n",
       " 'n',\n",
       " 'funni',\n",
       " '...',\n",
       " 'got',\n",
       " 'person',\n",
       " 'stori',\n",
       " 'daddi',\n",
       " 'bb',\n",
       " '.',\n",
       " 'take',\n",
       " 'like',\n",
       " 'noon',\n",
       " 'want',\n",
       " 'go',\n",
       " '?',\n",
       " 'train',\n",
       " 'tomorrow',\n",
       " '?',\n",
       " 'farm',\n",
       " 'open',\n",
       " '?',\n",
       " 'dunno',\n",
       " 'ask',\n",
       " '.',\n",
       " 'still',\n",
       " 'work',\n",
       " '?',\n",
       " 'howz',\n",
       " 'person',\n",
       " 'stori',\n",
       " 'whore',\n",
       " 'unbeliev',\n",
       " '.',\n",
       " 'like',\n",
       " 'person',\n",
       " 'size',\n",
       " 'keep',\n",
       " 'away',\n",
       " 'like',\n",
       " 'ok.',\n",
       " 'c',\n",
       " '.',\n",
       " 'mean',\n",
       " 'fat',\n",
       " 'head',\n",
       " 'ya',\n",
       " 'came',\n",
       " 'ago',\n",
       " 'oh',\n",
       " 'sorri',\n",
       " 'pleas',\n",
       " 'practis',\n",
       " 'curtsey',\n",
       " '?',\n",
       " 'town',\n",
       " 'alon',\n",
       " '?',\n",
       " 'onlin',\n",
       " 'transact',\n",
       " '?',\n",
       " 'say',\n",
       " 'thanks2',\n",
       " '.',\n",
       " 'heard',\n",
       " 'week',\n",
       " '?',\n",
       " 'normal',\n",
       " ':',\n",
       " ')',\n",
       " 'number',\n",
       " 'vivek',\n",
       " '..',\n",
       " 'dont',\n",
       " 'use',\n",
       " 'hook',\n",
       " 'receiv',\n",
       " 'msg',\n",
       " '?',\n",
       " 'home',\n",
       " 'lei',\n",
       " '...',\n",
       " 'hope',\n",
       " 'scare',\n",
       " '!',\n",
       " 'seem',\n",
       " 'unnecessarili',\n",
       " 'hostil',\n",
       " 'yes',\n",
       " 'appt',\n",
       " '?',\n",
       " 'anytim',\n",
       " 'you_know',\n",
       " '...',\n",
       " 'went',\n",
       " 'project',\n",
       " 'centr',\n",
       " 'get',\n",
       " 'back',\n",
       " 'home',\n",
       " 'ok',\n",
       " 'way',\n",
       " 'railway',\n",
       " 'someth',\n",
       " 'ate',\n",
       " '?',\n",
       " 'hank',\n",
       " 'lotsli',\n",
       " '!',\n",
       " 'awesom',\n",
       " ',',\n",
       " 'minut',\n",
       " 'abl',\n",
       " 'sleep',\n",
       " '.',\n",
       " 'anyth',\n",
       " 'you_know',\n",
       " '.',\n",
       " 'yeah',\n",
       " \"n't\",\n",
       " 'see',\n",
       " 'k.then',\n",
       " 'special',\n",
       " '?',\n",
       " 'came',\n",
       " 'hostel',\n",
       " '.',\n",
       " 'guy',\n",
       " 'leav',\n",
       " '?',\n",
       " 'abl',\n",
       " 'anyth',\n",
       " '.',\n",
       " 'alright',\n",
       " 'new',\n",
       " 'goal',\n",
       " 'cos',\n",
       " 'want',\n",
       " 'thing',\n",
       " 'headin',\n",
       " 'toward',\n",
       " 'busetop',\n",
       " 'most',\n",
       " 'like',\n",
       " '.',\n",
       " 'laptop',\n",
       " 'take',\n",
       " '.',\n",
       " 'time',\n",
       " 'come',\n",
       " 'tomorrow',\n",
       " 'morn',\n",
       " 'ok',\n",
       " '.',\n",
       " ':',\n",
       " '(',\n",
       " '....',\n",
       " 'mm',\n",
       " 'food',\n",
       " 'buddi',\n",
       " 'dear',\n",
       " 'reach',\n",
       " 'room',\n",
       " 'parti',\n",
       " 'alex',\n",
       " 'nichol',\n",
       " 'come',\n",
       " 'tomorrow',\n",
       " 'di',\n",
       " 'check',\n",
       " 'nuerologist',\n",
       " '.',\n",
       " 'time',\n",
       " 'get',\n",
       " '?',\n",
       " 'famili',\n",
       " 'happi',\n",
       " '..',\n",
       " 'mom',\n",
       " 'want',\n",
       " 'know',\n",
       " 'slept',\n",
       " 'time.you',\n",
       " '?',\n",
       " 'lem',\n",
       " 'know',\n",
       " 're',\n",
       " '?',\n",
       " 'miss',\n",
       " '!',\n",
       " 'rememb',\n",
       " 'day',\n",
       " '..',\n",
       " 'great',\n",
       " '!',\n",
       " '?',\n",
       " 'go',\n",
       " 'bed',\n",
       " 'prin',\n",
       " 'offic',\n",
       " 'na',\n",
       " '.',\n",
       " 're',\n",
       " 'done',\n",
       " 'mean',\n",
       " 'wait',\n",
       " 'call',\n",
       " '.',\n",
       " 'open',\n",
       " 'door',\n",
       " '?',\n",
       " \"'s\",\n",
       " 'husband',\n",
       " '.',\n",
       " 'work',\n",
       " 'right',\n",
       " '?',\n",
       " 'come',\n",
       " 'tomorrow',\n",
       " 'di',\n",
       " 'wet',\n",
       " 'right',\n",
       " '?',\n",
       " 'attend',\n",
       " 'noth',\n",
       " '.',\n",
       " '*',\n",
       " 'septemb',\n",
       " '!',\n",
       " 'sever',\n",
       " 'sir',\n",
       " '.',\n",
       " 'yes',\n",
       " 'chat',\n",
       " '.',\n",
       " 'll',\n",
       " 'late',\n",
       " '...',\n",
       " 'much',\n",
       " 'eighth',\n",
       " '?',\n",
       " 'arun',\n",
       " 'transfr',\n",
       " 'amt',\n",
       " \"'s\",\n",
       " 'signific',\n",
       " '?',\n",
       " 'ì_',\n",
       " 'come',\n",
       " '?',\n",
       " 'work',\n",
       " 'week',\n",
       " '?',\n",
       " 'ok',\n",
       " 'you_know',\n",
       " '...',\n",
       " 'cutest',\n",
       " 'girl',\n",
       " 'world',\n",
       " 'world',\n",
       " 'famamus',\n",
       " '....',\n",
       " 'anyth',\n",
       " 'lar',\n",
       " '...',\n",
       " 'yes',\n",
       " 'mani',\n",
       " 'sweet',\n",
       " 'make',\n",
       " 'fb',\n",
       " 'list',\n",
       " 'still',\n",
       " 'grinder',\n",
       " '?',\n",
       " 'drive',\n",
       " 'you_know',\n",
       " '.',\n",
       " 'see',\n",
       " 'half',\n",
       " 'hour',\n",
       " 'ew',\n",
       " 'one',\n",
       " '?',\n",
       " 'dare',\n",
       " 'chang',\n",
       " 'ring',\n",
       " 'still',\n",
       " 'game',\n",
       " '?',\n",
       " \"'s\",\n",
       " 'weather',\n",
       " '?',\n",
       " 'lmao',\n",
       " 'fun',\n",
       " '...',\n",
       " 'still',\n",
       " 'tonight',\n",
       " '?',\n",
       " 'webpag',\n",
       " 'avail',\n",
       " '!',\n",
       " 'okay',\n",
       " 'thought',\n",
       " 'expert',\n",
       " 'buddi',\n",
       " 'car',\n",
       " 'park',\n",
       " 'creepi',\n",
       " 'crazi',\n",
       " '.',\n",
       " 'remind',\n",
       " 'hrs',\n",
       " '.',\n",
       " 'glad',\n",
       " 'talk',\n",
       " '.',\n",
       " 'also',\n",
       " \"'s\",\n",
       " 'piec',\n",
       " 're',\n",
       " 'right',\n",
       " 'think',\n",
       " 'yo',\n",
       " 'guess',\n",
       " 'drop',\n",
       " 'pussi',\n",
       " 'perfect',\n",
       " '!',\n",
       " 'batteri',\n",
       " 'low',\n",
       " 'babe',\n",
       " 'reach',\n",
       " 'ten',\n",
       " 'morn',\n",
       " 'mm',\n",
       " 'way',\n",
       " 'railway',\n",
       " 'tall',\n",
       " 'princess',\n",
       " '?',\n",
       " 'also',\n",
       " 'maaaan',\n",
       " 'miss',\n",
       " 'gobi',\n",
       " 'art',\n",
       " 'colleg',\n",
       " 'prakesh',\n",
       " 'know',\n",
       " '.',\n",
       " 'hello',\n",
       " 'madam',\n",
       " '?',\n",
       " 're',\n",
       " 'done',\n",
       " '...',\n",
       " 'outsid',\n",
       " 'offic',\n",
       " 'take',\n",
       " 'annoy',\n",
       " \"n't\",\n",
       " '.',\n",
       " 'ok',\n",
       " 'you_know',\n",
       " '...',\n",
       " 'room',\n",
       " '?',\n",
       " 'need',\n",
       " 'much',\n",
       " 'get',\n",
       " '?',\n",
       " 'packag',\n",
       " 'program',\n",
       " 'well',\n",
       " 'noth',\n",
       " '.',\n",
       " '...',\n",
       " 'outsid',\n",
       " 'alreadi',\n",
       " '.',\n",
       " 'yup',\n",
       " 'ok',\n",
       " '...',\n",
       " 'pick',\n",
       " 'various',\n",
       " 'point',\n",
       " 'plan',\n",
       " 'pongal',\n",
       " '?',\n",
       " 'still',\n",
       " 'custom',\n",
       " 'place',\n",
       " 'send',\n",
       " 'id',\n",
       " 'password',\n",
       " '*',\n",
       " 'wear',\n",
       " '?',\n",
       " 'ok',\n",
       " 'you_know',\n",
       " '...',\n",
       " 'howz',\n",
       " 'person',\n",
       " 'stori',\n",
       " 'nice.nice.how',\n",
       " 'work',\n",
       " '?',\n",
       " 'ok',\n",
       " 'thanx',\n",
       " '...',\n",
       " 'see',\n",
       " 'half',\n",
       " 'hour',\n",
       " 'custom',\n",
       " 'place',\n",
       " 'call',\n",
       " 'ok',\n",
       " 'shall',\n",
       " 'talk',\n",
       " 'account',\n",
       " 'number',\n",
       " '?',\n",
       " 'danc',\n",
       " 'river',\n",
       " '?',\n",
       " 'manag',\n",
       " 'puzzel',\n",
       " '.',\n",
       " 'sure',\n",
       " 'stomach',\n",
       " '...',\n",
       " 'hang',\n",
       " 'brother',\n",
       " 'famili',\n",
       " 'road',\n",
       " 'cant',\n",
       " 'txt',\n",
       " 'thank',\n",
       " 'meet',\n",
       " 'monday',\n",
       " 'dled',\n",
       " '3d',\n",
       " 'imp',\n",
       " 'dint',\n",
       " 'touch',\n",
       " '.',\n",
       " 'anyth',\n",
       " 'you_know',\n",
       " '...',\n",
       " 'know',\n",
       " 'mood',\n",
       " 'today',\n",
       " 'see',\n",
       " '?',\n",
       " 'thought',\n",
       " 'oh',\n",
       " 'ok',\n",
       " '..',\n",
       " 'mean',\n",
       " 'get',\n",
       " 'door',\n",
       " 'collect',\n",
       " 'laptop',\n",
       " '?',\n",
       " 'sleep',\n",
       " '..',\n",
       " 'surf',\n",
       " 'see',\n",
       " 'requir',\n",
       " 'pleas',\n",
       " 'came',\n",
       " 'hostel',\n",
       " '.',\n",
       " 'know',\n",
       " 'result',\n",
       " '.',\n",
       " 'spare',\n",
       " 'power',\n",
       " 'suppli',\n",
       " 'll',\n",
       " 'meet',\n",
       " 'lobbi',\n",
       " 'didnt',\n",
       " 'holla',\n",
       " '?',\n",
       " 'happen',\n",
       " 'dear',\n",
       " 'tell',\n",
       " 'audri',\n",
       " 'lousi',\n",
       " 'autocorrect',\n",
       " 'dont',\n",
       " 'messag',\n",
       " 'offer',\n",
       " 'spoon',\n",
       " 'okay',\n",
       " '?',\n",
       " 'remind',\n",
       " 'get',\n",
       " 'shall',\n",
       " 'ill',\n",
       " 'obey',\n",
       " '.',\n",
       " 'seem',\n",
       " 'unnecessarili',\n",
       " 'affection',\n",
       " 'howz',\n",
       " 'person',\n",
       " 'stori',\n",
       " 'welp',\n",
       " 'appar',\n",
       " 'retir',\n",
       " 'curious',\n",
       " 'cuz',\n",
       " 'ask',\n",
       " 'nice.nice.how',\n",
       " 'work',\n",
       " '?',\n",
       " 'home',\n",
       " 'also',\n",
       " '.',\n",
       " \"'s\",\n",
       " 'pin',\n",
       " '?',\n",
       " 'samus',\n",
       " 'shoulder',\n",
       " 'yet',\n",
       " 'shore',\n",
       " 'takin',\n",
       " 'bus',\n",
       " 'thanx',\n",
       " 'lot',\n",
       " '...',\n",
       " 'happen',\n",
       " 'interview',\n",
       " '?',\n",
       " \"'m\",\n",
       " 'done',\n",
       " '...',\n",
       " 'happen',\n",
       " 'tell',\n",
       " 'truth',\n",
       " 'sorri',\n",
       " 'hurt',\n",
       " '.',\n",
       " 'time',\n",
       " 'come',\n",
       " '.',\n",
       " 'ask',\n",
       " 'princess',\n",
       " '?',\n",
       " 'ok',\n",
       " 'you_know',\n",
       " '.',\n",
       " 'gibb',\n",
       " 'unsold.mik',\n",
       " 'hussey',\n",
       " 'how',\n",
       " 'watch',\n",
       " 'resiz',\n",
       " 'dude',\n",
       " 'go',\n",
       " 'sup',\n",
       " 'cook',\n",
       " 'dinner',\n",
       " '?',\n",
       " 'ok',\n",
       " 'thanx',\n",
       " '...',\n",
       " 'heehe',\n",
       " 'funni',\n",
       " 'tho',\n",
       " 'jus',\n",
       " 'chillaxin',\n",
       " ',',\n",
       " 'fine',\n",
       " '.',\n",
       " 'thank',\n",
       " 'much',\n",
       " 'got',\n",
       " 'clean',\n",
       " 'yeah',\n",
       " 'get',\n",
       " 'unlimit',\n",
       " 'ok',\n",
       " 'prob',\n",
       " '...',\n",
       " '...',\n",
       " 'pub',\n",
       " '?',\n",
       " 'want',\n",
       " 'go',\n",
       " 'perumbavoor',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'probabl',\n",
       " 'part',\n",
       " 'check',\n",
       " 'iq',\n",
       " \"n't\",\n",
       " 'pay',\n",
       " 'attent',\n",
       " 'ill',\n",
       " 'b',\n",
       " 'soon',\n",
       " 'dear',\n",
       " 'good',\n",
       " 'morn',\n",
       " \"'s\",\n",
       " 'paper',\n",
       " '?',\n",
       " 'arun',\n",
       " 'transfr',\n",
       " 'amt',\n",
       " 'oh',\n",
       " 'charg',\n",
       " 'camera',\n",
       " \"'m\",\n",
       " 'home',\n",
       " '.',\n",
       " 'got',\n",
       " 'person',\n",
       " 'stori',\n",
       " 'ok',\n",
       " 'you_know',\n",
       " '.',\n",
       " 'sleep',\n",
       " '..',\n",
       " 'surf',\n",
       " 'network',\n",
       " 'job',\n",
       " '.',\n",
       " 'eatin',\n",
       " 'lunch',\n",
       " '...',\n",
       " 'yup',\n",
       " 'ok',\n",
       " '...',\n",
       " 'dear',\n",
       " '.',\n",
       " 'call',\n",
       " 'haha',\n",
       " ',',\n",
       " 'thinkin',\n",
       " 'donno',\n",
       " 'gene',\n",
       " 'someth',\n",
       " 'come',\n",
       " 'funer',\n",
       " 'home',\n",
       " 'got',\n",
       " 'person',\n",
       " 'stori',\n",
       " 'drive',\n",
       " 'train',\n",
       " '?',\n",
       " 'make',\n",
       " 'happi',\n",
       " ...]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [words for lists in sorted_df.tokenized_data for words in lists]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68098 7993\n"
     ]
    }
   ],
   "source": [
    "print(len(words),len(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_length\n",
       "5      481\n",
       "6      432\n",
       "4      395\n",
       "7      387\n",
       "8      364\n",
       "      ... \n",
       "61       1\n",
       "58       1\n",
       "43       1\n",
       "49       1\n",
       "202      1\n",
       "Name: count, Length: 66, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df['word_length'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target          0\n",
       "Text_Message    0\n",
       "clean_data      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# def frequency_count(df):\n",
    "#     word = []\n",
    "#     for text in df:\n",
    "#         sentenc = text.split()\n",
    "#         for i in sentenc:\n",
    "#             word.append(i)\n",
    "#     freq = Counter(word)\n",
    "#     word_freq = pd.DataFrame(freq.items(),columns=['word','frequency'])\n",
    "#     sf = word_freq.sort_values(by='frequency')\n",
    "#     return sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def frequency_count(df):\n",
    "    word_list = []\n",
    "\n",
    "    for text in df:\n",
    "        tokens = word_tokenize(text)\n",
    "        word_list.extend(tokens)\n",
    "\n",
    "    freq = Counter(word_list)\n",
    "    word_freq = pd.DataFrame(freq.items(), columns=['word', 'frequency'])\n",
    "    sf = word_freq.sort_values(by='frequency', ascending=False)\n",
    "\n",
    "    return sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = frequency_count(df.clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"frequency_count\",exist_ok= True)\n",
    "save_data(sf, os.path.join(\"frequency_count\",\"data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>.</td>\n",
       "      <td>4769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>1872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>?</td>\n",
       "      <td>1541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>!</td>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>...</td>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>;</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>:</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>..</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>call</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  frequency\n",
       "84      .       4769\n",
       "3       ,       1872\n",
       "74      ?       1541\n",
       "69      !       1381\n",
       "14    ...       1131\n",
       "45      &        916\n",
       "283     ;        764\n",
       "169     :        717\n",
       "5      ..        681\n",
       "114  call        656"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[['Target','clean_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>clean_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go jurong point , crazi .. avail bugi n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar ... joke oni ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entri 2 week comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dun say earli hor ... c alreadi say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah n't think goe usf , live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>2nd time tri 2 contact . å£750 pound prize . 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>ì_ b go esplanad fr home ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>piti , * mood . ... suggest ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>guy bitch act like 'd interest buy someth els ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>roll floor laugh . true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target                                         clean_data\n",
       "0          0  go jurong point , crazi .. avail bugi n great ...\n",
       "1          0                            ok lar ... joke oni ...\n",
       "2          1  free entri 2 week comp win fa cup final tkts 2...\n",
       "3          0            dun say earli hor ... c alreadi say ...\n",
       "4          0         nah n't think goe usf , live around though\n",
       "...      ...                                                ...\n",
       "5567       1  2nd time tri 2 contact . å£750 pound prize . 2...\n",
       "5568       0                         ì_ b go esplanad fr home ?\n",
       "5569       0                      piti , * mood . ... suggest ?\n",
       "5570       0  guy bitch act like 'd interest buy someth els ...\n",
       "5571       0                       roll floor laugh . true name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_words = 5000  # Adjust based on your vocabulary size\n",
    "max_len = 60  # Adjust based on your sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(df['clean_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.text.Tokenizer at 0x1e9aba9dc10>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df['clean_data'])\n",
    "padded_sequences = pad_sequences(sequences, \n",
    "                                 maxlen=max_len,\n",
    "                                 truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go jurong point , crazi .. avail bugi n great world la e buffet ... cine got amor wat ...'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.clean_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words =  20\n",
    "len_word = 5\n",
    "tokenizer = Tokenizer(num_words=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.fit_on_texts(new_df.clean_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(new_df.clean_data[0])\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(new_df.clean_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = pad_sequences(sequences, \n",
    "                                 maxlen=max_len,\n",
    "                                 truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>clean_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0 1 ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>go jurong point, crazy ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ok free entry week comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ok dun say early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ok nah think goes usf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                 clean_data\n",
       "0       0                     0 1 ok\n",
       "1       0  go jurong point, crazy ok\n",
       "2       1    ok free entry week comp\n",
       "3       0           ok dun say early\n",
       "4       1      ok nah think goes usf"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = {'Target': [0,0, 1, 0, 1],\n",
    "        'clean_data': [\"0 1 ok\",'go jurong point, crazy ok', ' ok free entry week comp', 'ok dun say early', 'ok nah think goes usf']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', '1', 'ok'], ['go', 'jurong', 'point', ',', 'crazy', 'ok'], ['ok', 'free', 'entry', 'week', 'comp'], ['ok', 'dun', 'say', 'early'], ['ok', 'nah', 'think', 'goes', 'usf']]\n",
      "[['0', '1', 'ok'], ['go', 'jurong', 'point,', 'crazy', 'ok'], ['', 'ok', 'free', 'entry', 'week', 'comp'], ['ok', 'dun', 'say', 'early'], ['ok', 'nah', 'think', 'goes', 'usf']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print([word_tokenize(sen) for sen in df['clean_data'] ])\n",
    "print([(sen.split(' ')) for sen in df['clean_data'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>clean_data</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0 1 ok</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>go jurong point, crazy ok</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ok free entry week comp</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ok dun say early</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ok nah think goes usf</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                 clean_data  word_count\n",
       "0       0                     0 1 ok           3\n",
       "1       0  go jurong point, crazy ok           5\n",
       "2       1    ok free entry week comp           6\n",
       "3       0           ok dun say early           4\n",
       "4       1      ok nah think goes usf           5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_count'] = [len(sen.split(' ')) for sen in df['clean_data'] ]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = [word for sen in df['clean_data'] for word in sen.split()]\n",
    "len(all_words),len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 16 \n",
    "max_len = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['clean_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'ok',\n",
       " 2: '0',\n",
       " 3: '1',\n",
       " 4: 'go',\n",
       " 5: 'jurong',\n",
       " 6: 'point',\n",
       " 7: 'crazy',\n",
       " 8: 'free',\n",
       " 9: 'entry',\n",
       " 10: 'week',\n",
       " 11: 'comp',\n",
       " 12: 'dun',\n",
       " 13: 'say',\n",
       " 14: 'early',\n",
       " 15: 'nah',\n",
       " 16: 'think',\n",
       " 17: 'goes',\n",
       " 18: 'usf'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {2: 1,\n",
       "             1: 5,\n",
       "             3: 1,\n",
       "             7: 1,\n",
       "             6: 1,\n",
       "             4: 1,\n",
       "             5: 1,\n",
       "             9: 1,\n",
       "             8: 1,\n",
       "             10: 1,\n",
       "             11: 1,\n",
       "             14: 1,\n",
       "             13: 1,\n",
       "             12: 1,\n",
       "             15: 1,\n",
       "             18: 1,\n",
       "             17: 1,\n",
       "             16: 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0', 1),\n",
       "             ('1', 1),\n",
       "             ('ok', 5),\n",
       "             ('go', 1),\n",
       "             ('jurong', 1),\n",
       "             ('point', 1),\n",
       "             ('crazy', 1),\n",
       "             ('free', 1),\n",
       "             ('entry', 1),\n",
       "             ('week', 1),\n",
       "             ('comp', 1),\n",
       "             ('dun', 1),\n",
       "             ('say', 1),\n",
       "             ('early', 1),\n",
       "             ('nah', 1),\n",
       "             ('think', 1),\n",
       "             ('goes', 1),\n",
       "             ('usf', 1)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ok': 1,\n",
       " '0': 2,\n",
       " '1': 3,\n",
       " 'go': 4,\n",
       " 'jurong': 5,\n",
       " 'point': 6,\n",
       " 'crazy': 7,\n",
       " 'free': 8,\n",
       " 'entry': 9,\n",
       " 'week': 10,\n",
       " 'comp': 11,\n",
       " 'dun': 12,\n",
       " 'say': 13,\n",
       " 'early': 14,\n",
       " 'nah': 15,\n",
       " 'think': 16,\n",
       " 'goes': 17,\n",
       " 'usf': 18}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 1], [4, 5, 6, 7, 1], [1, 8, 9, 10, 11], [1, 12, 13, 14], [1, 15]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df['clean_data'])\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  5,  6,  7,  1,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0],\n",
       "       [ 1, 12, 13, 14,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 15,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, maxlen=10,truncating='post',padding=\"post\")\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>clean_data</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0 1 ok</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>go jurong point, crazy ok</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ok free entry week comp</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ok dun say early</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ok nah think goes usf</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                 clean_data  word_count\n",
       "0       0                     0 1 ok           3\n",
       "1       0  go jurong point, crazy ok           5\n",
       "2       1    ok free entry week comp           6\n",
       "3       0           ok dun say early           4\n",
       "4       1      ok nah think goes usf           5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization and padding\n",
    "max_words = 20  # Adjust based on your vocabulary size\n",
    "max_len = 5  # Adjust based on your sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['clean_data'])\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_data'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Encoding target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['Target'] = label_encoder.fit_transform(df['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer1.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer1.pkl', 'rb') as tokenizer_file:\n",
    "    loaded_tokenizer = pickle.load(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5, 6, 7, 1]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = [\"\tgo jurong point, crazy ok\"]\n",
    "new_sequences = loaded_tokenizer.texts_to_sequences(new_text)\n",
    "new_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
